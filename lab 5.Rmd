---
title: 'Lab 4: Multivariate normal and LDA, QDA'
author: "Zhanrui Cai"
date: '2022-09-14'
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Exercise 1 Classification Trees

Bronchopulmonary Dysplasia (BPD) Study (Biostatistics Casebook, pp 104-119)

Training samples consist of all infants at the Stanford Medical Center between 1962 and 1973 who were diagnosed with respiratory distress syndrome (RDS) and received ventilatory assistance for at least 24 hours (except one infant with incomplete records). Most of these babies were born prematurely and had underdeveloped lungs. Some breathing assistance involving elevated levels of oxygen was needed to keep the babies alive. Bronchiopulmonary dysplasia (BPD) is deterioration of lung tissue (scarring) in infants exposed to a high level of oxygen. A panel of physicians reviewed each case to determine if BPD was present.

Infants who did not survive for at least 72 hours were excluded from the analysis because there was not enough time for BPD to develop. One additional infant was excluded due to incomplete records. This reduced the sample from 299 to 248 babies, including 78 with BPD and 170 without BPD.

General background variables:
Sex (0=female, 1=male) 
YOB: year of birth (coded from 62 to 73) 
APGAR: one minute APGAR score (0 to 10 with 10 as the most healthy) 
GEST: gestational age (weeks × 10) 
BWT: birth weight (grams) 
AGSYM: age at onset of RDS (hours × 10) 
RDS: severity of initial X-ray for RDS (0 to 5=most severe)
Therapy variables:
AGVEN: Age at onset of ventilation (hours) 
VENTL: total hours on the ventilator
LOWO2: hours of exposure to (21-39%) levels of oxygen 
MEDO2: hours of exposure to (40-79%) levels of oxygen
HIO2: hours of exposure to (80-100%) levels of oxygen 
INTUB: hours of endotracheal intubation
Response variable: BPD (coded 1=yes, 2=no)

0. Attach the rpart and rpart.plot packages.

```{r}
set.seed(123)
library(rpart)
library(rpart.plot)
```

1. Construct a classification tree. First convert categorical variables like sex and rds to factors. (There is no need to convert categorical variables into zero-one variables, because it would not change the tree that is produced. Read the data into a data frame and create factors.)
```{r}
bpdr <- read.csv("bpd.csv", header=T)
bpdr$sex <- as.factor(bpdr$sex)
head(bpdr)

```

2. Create a factor to distinguish the two populations with labels "BPD" and "No BPD".
```{r}

bpdr$y[bpdr$bpd==1]<-'BPD'
bpdr$y[bpdr$bpd==2]<-'No BPD'
bpdr$y<-as.factor(bpdr$y)

head(bpdr)
```

3. Train the decision tree using the function rpart(). Print out the summary of the tree. Which variable did the tree choose to split first?
Medo2
```{r}
bpd.rp <- rpart(y ~ sex+yob+gest+bwt+agsym+agven+intub+ventl+lowo2+medo2+hio2+rds, data=bpdr, cp=0.0001)

summary(bpd.rp)
```

4. Using function plot() to display the tree. The arguments of the par( ) function enable all of the text for the node labels to be displayed, otherwise only part of the label can be seen. How many terminal nodes does the tree have?
9 terminal nodes

```{r}
par(mfrow=c(1,1), xpd=NA)
plot(bpd.rp,uniform=T)
text(bpd.rp, use.n=TRUE, cex=0.8)
title("Classification Tree")

```


5. Estimate the predicted probabilities of the two classes using the predict() function. Print the estimated probabilities for the first 10 samples using the head() function.
```{r}
bpd.prob <- predict(bpd.rp)
head(bpd.prob, 10)

##### Get AUC

library(pROC)
roc.curve = roc(bpdr$y, bpd.prob[,1])
auc(roc.curve)
```


6. Make a table of the confusion matrix. The rows are the truth. What is the true positive rate? What is the false discovery rate? What is the prediction accuracy rate?
58/65 is the true positive rate. 7/65 is the false discovery rate. 221/248 is the prediction accuracy rate.
```{r}
bpd.class <- rep( "Predicted BPD", nrow(bpd.prob))
bpd.class[ bpd.prob[ ,2] > 0.5] <- "Predicted No BPD"
table(bpdr$y, bpd.class)
```

7. Display the tree using the function rpart.plot().
```{r}
rpart.plot(bpd.rp)
```

8. Based on 7, what is the predicted probability of having Bronchopulmonary Dysplasia if medo2>183 and ventl<220.5? What is the predicted probability of not having Bronchopulmonary Dysplasia if medo2<183 and hio2>159.5 and medo2<45?
a) 19 percent
b).62 

9. Based on 7, what is the percentage of samples satisfy that medo2>183? What is the percentage of samples that satisfy medo2<183 and hio2>160?
25 percent
9 percent
## Exercise 2 Out-of-sample prediction

In this exercise, we will learn how to perform training and testing on different datasets, i.e., out-of-sample prediction.

1. Create sub-datasets of bpd: bpd1 and bpd2, whose response $y$ is equal to 1 and 2, respectively.
```{r}
bpd1 <- subset(bpdr, bpdr$bpd == 1)
bpd1$rds <- as.factor(bpd1$rds)
bpd2 <- subset(bpdr, bpdr$bpd == 2)
bpd2$rds <- as.factor(bpd2$rds)

```

2. What is the sample size of bpd1 and bpd2? Report them as n1 and n2, respectively.
```{r}
n1 <- dim(bpd1)[1]
n2 <- dim(bpd2)[1]
```

3. Draw a random subset of bpd1 of size n1/2, name as bpd1.train. Name the remaining subset of bpd1 as bpd1.test. 
```{r}
sample1 <- sample(nrow(bpd1), size = n1/2, replace = F)
bpd1.train <- bpd1[sample1, ]
bpd1.test <- bpd1[-sample1, ]
```

4. Draw a random subset of bpd2 of size n2/2, name as bpd2.train. Name the remaining subset of bpd2 as bpd2.test. 
```{r}
sample2 <- sample(nrow(bpd2), size = n2/2, replace = F)
bpd2.train <- bpd2[sample2, ]
bpd2.test <- bpd2[-sample2, ]
```

5. Use the function rbind() to combine bpd1.train. and bpd2.train, name it as training.set.
```{r}
training.set <- rbind(bpd1.train, bpd2.train)
```

6. Use the function rbind() to combine bpd1.test. and bpd2.test, name it as testing.set.
```{r}
testing.set <- rbind(bpd1.test, bpd2.test)

```

7. Use the training.set to train the classification tree (with the function rpart). Predict the probabilities of the testing.set.
```{r}
bpd.tree <- rpart(y ~ sex+yob+gest+bwt+agsym+agven+intub+ventl+lowo2+medo2+hio2+rds, data=training.set, cp=0.0001)

```

8. What is the prediction accuracy?
```{r}
bpd.prob <- predict(bpd.tree, testing.set)

y = as.numeric(testing.set$y)-1
mean((bpd.prob[,2]>0.5)==y)
```
9. What is the AUC?
```{r}
library(pROC)
roc.curve = roc(testing.set$y, bpd.prob[,1])
auc(roc.curve)
```

10. Write a "for loop" to repeat the process in 1-8 for 20 times, report the mean of prediction accuracy.
```{r}
accuracy_all = rep(0, 20)
auc_all = rep(0, 20)

for(ii in 1:20){
  
  #### Fill in the code here
  
  accuracy_all[ii] = mean((bpd.prob[,2]>0.5)==y)
  auc_all[ii] = auc(roc.curve)
}

```


## Exercise 3 Random Forests

We will use the same dataset as in exercise 1. We will use the R package randomForest.

```{r}
library(randomForest)
bpdr1 <- na.omit(bpdr)

bpd31 <- subset(bpdr1, bpdr1$bpd == 1)
bpd31$rds <- as.factor(bpd31$rds)
bpd32 <- subset(bpdr1, bpdr1$bpd == 2)
bpd32$rds <- as.factor(bpd32$rds)
num1 <- dim(bpd31)[1]
num2 <- dim(bpd32)[1]
sample31 <- sample(nrow(bpd31), size = num1/2, replace = F)
bpd31.train <- bpd31[sample31, ]
bpd31.test <- bpd31[-sample31, ]
sample32 <- sample(nrow(bpd32), size = num2/2, replace = F)
bpd32.train <- bpd32[sample32, ]
bpd32.test <- bpd32[-sample32, ]
training.set1 <- rbind(bpd31.train, bpd32.train)
testing.set1 <- rbind(bpd31.test, bpd32.test)
```

1. Train the random forest model using the function randomForest(). Set the number of trees to be 50.
```{r}
#### Fit random forest model, omit the NA missing values in the data
rf1 = randomForest(y ~ sex+yob+gest+bwt+agsym+agven+intub+ventl+lowo2+medo2+hio2+rds, ntree = 50,
                   data=bpdr1, na.action=na.omit)
```

2. Estimate the predicted probabilities of the two classes using the predict() function. Print the estimated probabilities for the first 10 samples using the head() function.
```{r}
bpd.rp1 <- predict(rf1, type = "prob", data = testing.set1)
head(bpd.rp1, 10)
y = as.numeric(bpdr1$y)-1
mean((bpd.rp1[,2]>0.5)==y)

##### Get AUC
library(pROC)
roc.curve = roc(bpdr1$y, bpd.rp1[,1])
auc(roc.curve)
```

3. Make a table of the confusion matrix. The rows are the truth. What is the true positive rate? What is the false discovery rate? What is the prediction accuracy rate?
41/51 is the true positive rate. 10/51 is the false discovery rate and 174/197 is the prediction accuracy rate. 
```{r}
bpd.class1 <- rep( "Predicted BPD", nrow(bpd.rp1))
bpd.class1[ bpd.rp1[ ,2] > 0.5] <- "Predicted No BPD"
table(bpdr1$y, bpd.class1)
```

4. Write a for loop as in exercise 2.9, but use random forest as the prediction model. Report the mean of prediction accuracy. Is it higher or lower compared to the single classification tree?
```{r}
accuracy_all1 = rep(0, 20)
auc_all1 = rep(0, 20)

for(ii in 1:20){
  
  #### Fill in the code here
  
  accuracy_all1[ii] = mean((bpd.rp1[,2]>0.5)==y)
  auc_all1[ii] = auc(roc.curve)
}
```

5. Change the number of trees in (4) to be 25. Does the prediction accuracy increase? 

```{r}
rf1 = randomForest(y ~ sex+yob+gest+bwt+agsym+agven+intub+ventl+lowo2+medo2+hio2+rds, ntree = 25,
                   data=bpdr1, na.action=na.omit)

```




